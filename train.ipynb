{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers Pillow torch scipy matplotlib torchvision datasets diffusers accelerate vector_quantize_pytorch pytube moviepy torchmetrics timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ImMuTe(Dataset):\n",
    "\n",
    "    def __init__(self, images_path, captions_json_file, audios_path, start=0, end=2999, sampling_rate=16000, pixel=64, normalize=False):\n",
    "        super().__init__()\n",
    "        self.images_path = images_path\n",
    "        self.captions_json_file = captions_json_file\n",
    "        self.audios_path = audios_path\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # define transform to convert image to tensors\n",
    "        transform = [\n",
    "            transforms.Resize(pixel),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        if normalize:\n",
    "          transform.append(transforms.Normalize([0.5], [0.5]))\n",
    "\n",
    "        self.transform = transforms.Compose(transform)\n",
    "\n",
    "        # load captions\n",
    "        with open(self.captions_json_file, \"r\", encoding='utf-8') as f:\n",
    "            caps_dict = json.load(f)\n",
    "\n",
    "        # preload all data in a dict\n",
    "        self.all_data = {}\n",
    "        index = 0\n",
    "        for i in range(len(caps_dict)):\n",
    "            if i < start:\n",
    "              continue\n",
    "            if i > end:\n",
    "              break\n",
    "            try:\n",
    "              # load audio\n",
    "              wav, sr = torchaudio.load(f\"{self.audios_path}/aud_{i}.wav\")\n",
    "              wav = torchaudio.functional.resample(wav, orig_freq=sr, new_freq=self.sampling_rate)\n",
    "              wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "              if wav.size(-1) < self.sampling_rate * 10:\n",
    "                  pad_len = self.sampling_rate * 10 - wav.size(-1)\n",
    "                  wav = torchfunc.pad(wav, (0, pad_len))\n",
    "\n",
    "              # transform image\n",
    "              img = Image.open(f\"{self.images_path}/test_{i}.png\")\n",
    "              img = self.transform(img)\n",
    "\n",
    "              # index image-text pair and save them to dict\n",
    "              self.all_data[index] = (wav, img, caps_dict[str(i)])\n",
    "              index += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # get total length of dataset\n",
    "        length = len(self.all_data)\n",
    "\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # get image-text pair by index\n",
    "        wav, img, txt = self.all_data[idx]\n",
    "\n",
    "        return (wav, img, txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoImageProcessor, AutoModel\n",
    "\n",
    "musicgen_proc = AutoProcessor.from_pretrained(\"facebook/musicgen-small\", low_cpu_mem_usage=True)\n",
    "vit_proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\", low_cpu_mem_usage=True)\n",
    "\n",
    "i2m = AutoModel.from_pretrained(\"juliagsy/imagic\", trust_remote_code=True)\n",
    "i2m.model.load_state_dict(torch.load(\"imagic_state_dict.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "immute = ImMuTe(\"images\", \"caption.json\", \"audios\", start=0, end=5000, sampling_rate=32000, pixel=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immute_ds = DataLoader(immute, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from IPython.display import Audio\n",
    "from IPython.core.display import display\n",
    "\n",
    "i2m.to(\"cuda\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(list(i2m.model.musicgen.decoder.lm_heads.parameters()) + list(i2m.model.img_lin.parameters()), lr=1e-05, weight_decay=1e-02)\n",
    "\n",
    "for epoch in range(0, 32):\n",
    "    losses = []\n",
    "    for b in immute_1_ds:\n",
    "        wav, img, _ = b\n",
    "        wav = wav.squeeze(1).tolist()\n",
    "\n",
    "        img = vit_proc(img, do_rescale=False, return_tensors=\"pt\")\n",
    "        wav = musicgen_proc(\n",
    "            audio=wav,\n",
    "            sampling_rate=32000,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        wav = wav.to(\"cuda\")\n",
    "        img = img.to(\"cuda\")\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = i2m.forward(img, wav)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(list(i2m.model.musicgen.decoder.lm_heads.parameters()) + list(i2m.model.img_lin.parameters()), 0.5)\n",
    "        opt.step()\n",
    "\n",
    "    print(f\"epoch {epoch}: {np.mean(losses)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
